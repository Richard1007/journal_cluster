{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a4a95c-b667-470b-b066-91d83d05633c",
   "metadata": {},
   "source": [
    "Reviewers vragen sentiment-gebaseerde analyses. We voegen voor alle woorden in de dataset dit sentiment toe:\n",
    "\n",
    "    ✓ LIWC\n",
    "    ✓ SentiWordNet (NLTK  https://github.com/aesuli/SentiWordNet)\n",
    "    ✓ GI (General Inquirer)\n",
    "    ✓ Sentic https://sentic.net/\n",
    "    ? ANEW --> requested\n",
    "    ? EL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa5b29-faf9-422e-b718-0ca5538b6347",
   "metadata": {},
   "source": [
    "### 1. Load wordlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b79eb42-18f8-4611-81b9-31b440228003",
   "metadata": {},
   "source": [
    "Code = first code-blocks of 1_Paper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1967694b-ae46-4ba0-9b72-ba59b8ff2f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring IP block and distribution channel\n",
      "Original data has 6145 rows.\n",
      "After deletion of rows with too few non-repetitive answers: 6145 rows\n",
      "Current cutoff is set to remove any count < 5\n",
      "Unrecognized word ratio (# nan / # cells): 14.55%\n",
      "After deletion of rows with too few (<=0) non-recognized answers: 6143 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy \n",
    "from spacy.util import compile_infix_regex\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import enchant\n",
    "\n",
    "data_file_path   = \"./data/survey-all.csv\"                      # filepath\n",
    "numberbatch_path = \"counterfitting/numberbatch-counterfitted\"  # ConceptNet model\n",
    "nlp = spacy.load(numberbatch_path)\n",
    "\n",
    "\n",
    "# Avoid splitting of dashes\n",
    "def custom_tokenizer(nlp):\n",
    "    inf = list(nlp.Defaults.infixes)               # Default infixes\n",
    "    inf.remove(r\"(?<=[0-9])[+\\-\\*^](?=[0-9-])\")    # Remove the generic op between numbers or between a number and a -\n",
    "    inf = tuple(inf)                               # Convert inf to tuple\n",
    "    infixes = inf + tuple([r\"(?<=[0-9])[+*^](?=[0-9-])\", r\"(?<=[0-9])-(?=-)\"])  # Add the removed rule after subtracting (?<=[0-9])-(?=[0-9]) pattern\n",
    "    infixes = [x for x in infixes if '-|–|—|--|---|——|~' not in x] # Remove - between letters rule\n",
    "    infix_re = compile_infix_regex(infixes)\n",
    "\n",
    "    return Tokenizer(nlp.vocab, prefix_search=None,\n",
    "                                suffix_search=None,\n",
    "                                infix_finditer=None,\n",
    "                                token_match=None,\n",
    "                                rules=None)\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "from lib.addons_enric import censor_ips\n",
    "from lib.hard_launch import load_data, remove_anomaly, process_text, construct_participant_level_df\n",
    "\n",
    "# less for testing\n",
    "df  = load_data(data_file_path)\n",
    "df = remove_anomaly(df)\n",
    "df = censor_ips(df)\n",
    "df,invalid_word,val_word_cnt,df_corrected_words = process_text(df,nlp)\n",
    "df = construct_participant_level_df(df)\n",
    "word_list   = list(val_word_cnt.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2616e-42ac-4484-bb87-e2fe1e50b5a7",
   "metadata": {},
   "source": [
    "## 2. Add sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7149834-9013-43e2-8b56-2b8280e68c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.hard_launch import P1_lemma, P2_lemma\n",
    "\n",
    "def combine(g):\n",
    "    return g.sum()\n",
    "\n",
    "df_words = df[[\"Source\"]+P1_lemma+P2_lemma].groupby(\"Source\").agg(combine).T\n",
    "df_words = df_words.sum()\n",
    "df_words = df_words.apply(lambda x: np.unique(x))\n",
    "\n",
    "srcs, vals = [],[]\n",
    "for i in df_words.index:\n",
    "    for w in df_words[i]:\n",
    "        srcs.append(i)\n",
    "        vals.append(w)\n",
    "\n",
    "df_words = pd.DataFrame(vals,srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8e136567-ec7c-4fcf-af13-e00869aff286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NLTK / SentiWordNet\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import numpy as np\n",
    "\n",
    "def score_sentiwordnet(word):\n",
    "    synset = list(swn.senti_synsets(word))\n",
    "    \n",
    "    if(len(list(synset)) > 0):\n",
    "        scores = list(synset)[0]\n",
    "        return [scores.neg_score(), scores.pos_score()]\n",
    "    return [0, 0]\n",
    "\n",
    "#### LIWC\n",
    "import liwc\n",
    "from collections import Counter\n",
    "\n",
    "parse, category_names = liwc.load_token_parser('./data/sentiment/LIWC2015_English.dic')\n",
    "def score_LIWC(word):\n",
    "    results = Counter(category for category in parse(word))\n",
    "    return [ \n",
    "        results['negemo (Negative Emotions)'],\n",
    "        results['posemo (Positive Emotions)']\n",
    "    ]\n",
    "\n",
    "#### General Inquiry\n",
    "#  http://www.wjh.harvard.edu/~inquirer/\n",
    "#  https://github.com/cran/SentimentAnalysis/blob/master/R/data.R\n",
    "GIpos = list(pd.read_csv('./data/sentiment/GIpos.csv')['x'])\n",
    "GIneg = list(pd.read_csv('./data/sentiment/GIneg.csv')['x'])\n",
    "def score_GI(word):\n",
    "    if(word.lower() in GIpos):\n",
    "        return [0,1]\n",
    "    if(word.lower() in GIneg):\n",
    "        return [1,0]\n",
    "    return [np.nan, np.nan]\n",
    "\n",
    "##### SenticNet\n",
    "from senticnet.senticnet import SenticNet\n",
    "sn = SenticNet()\n",
    "def score_senticnet(word):\n",
    "    try:\n",
    "        polarity_value = float(sn.polarity_value(word))\n",
    "    except:\n",
    "        return [np.nan, np.nan]\n",
    "    \n",
    "    if(polarity_value > 0):\n",
    "        return [0, polarity_value]\n",
    "    return [polarity_value,0]\n",
    "\n",
    "####### ANEW\n",
    "anew_list = pd.read_excel(\"data/sentiment/ANEW.xlsx\")\n",
    "anew_list = dict(zip(anew_list.word, anew_list.score))\n",
    "def score_anew(word):\n",
    "    if(word in anew_list):\n",
    "        return anew_list[word]\n",
    "    return np.nan\n",
    "\n",
    "###### Evaluative Lexicon\n",
    "EL_list = pd.read_csv(\"data/sentiment/EvaluativeLexicon20.csv\")\n",
    "EL_list = dict(zip(EL_list.Word, EL_list.Valence))\n",
    "def score_el(word):\n",
    "    if(word in EL_list):\n",
    "        return EL_list[word]\n",
    "    return np.nan\n",
    "\n",
    "def all_scores(word):\n",
    "    word = word.lower()\n",
    "    return np.r_[\n",
    "        score_LIWC(word),\n",
    "        score_sentiwordnet(word),\n",
    "        score_GI(word),\n",
    "        score_senticnet(word),\n",
    "        score_anew(word),\n",
    "        score_el(word)\n",
    "    ]\n",
    "sent_vars = [\"LIWC_neg\",\"LIWC_pos\",\"SentiWordNet_neg\",\"SentiWordNet_pos\",\"GI_neg\",\"GI_pos\",\"SenticNet_neg\",\"SenticNet_pos\",\"ANEW\",\"EL\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89321b46-1ccb-4442-8815-ca397b664e95",
   "metadata": {},
   "source": [
    "Apply score functions to the df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "62a47f71-58ec-43ea-a91a-9682ee6d37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words  = [ w[0] for w in df_words.values ]\n",
    "scores = [ all_scores(w) for w in words]\n",
    "df_scored = pd.DataFrame(scores, columns=sent_vars, index=words)\n",
    "df_scored[\"Source\"] = df_words.index\n",
    "df_scored.to_csv(\"output/corpus_sentiments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "18abf17d-1a9c-4085-8faa-26bac2f1c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.unique(words)).to_csv(\"output/unique_words.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e9442-7bb8-4177-aa04-e6ee697b4a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
